{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0204d948-83fe-43ec-980e-77642a53d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional config for better memory efficiency\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8a495d2-e8fd-4f90-bf66-c2966622c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from mapanything.models import MapAnything\n",
    "from mapanything.utils.image import load_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81fd79af-c22d-46a6-9999-83bf7b6759b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained dinov2_vitl14 from torch hub\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jpc/.cache/torch/hub/facebookresearch_dinov2_main\n"
     ]
    }
   ],
   "source": [
    "# Get inference device\n",
    "device = \"cpu\" \n",
    "\n",
    "# Init model - This requires internet access or the huggingface hub cache to be pre-downloaded\n",
    "# For Apache 2.0 license model, use \"facebook/map-anything-apache\"\n",
    "model = MapAnything.from_pretrained(\"facebook/map-anything\").to(device)\n",
    "\n",
    "# Load and preprocess images from a folder or list of paths\n",
    "#images = [\"/Users/jpc/Nextcloud/IST/Escolaridade/PIV2025/Datasets/TaagPIV/20251028_165814.jpg\" ] # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\n",
    "images = \"/Users/jpc/Nextcloud/IST/Escolaridade/PIV2025/AulasPraticas/aula7_3D_registration/plondres\" # or [\"path/to/img1.jpg\", \"path/to/img2.jpg\", ...]\n",
    "views = load_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ce45c6-3ea8-4459-8e37-1e70cca11e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jpc/Nextcloud/IST/code/aisp/map-anything/mapanything/models/mapanything/model.py:2052: UserWarning: bf16 is not supported on this device. Using fp16 instead.\n",
      "  warnings.warn(\n",
      "/Users/jpc/piton/lib/python3.13/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run inference\n",
    "predictions = model.infer(\n",
    "    views,                            # Input views\n",
    "    memory_efficient_inference=False, # Trades off speed for more views (up to 2000 views on 140 GB)\n",
    "    use_amp=True,                     # Use mixed precision inference (recommended)\n",
    "    amp_dtype=\"bf16\",                 # bf16 inference (recommended; falls back to fp16 if bf16 not supported)\n",
    "    apply_mask=True,                  # Apply masking to dense geometry outputs\n",
    "    mask_edges=True,                  # Remove edge artifacts by using normals and depth\n",
    "    apply_confidence_mask=False,      # Filter low-confidence regions\n",
    "    confidence_percentile=10,         # Remove bottom 10 percentile confidence pixels\n",
    ")\n",
    "\n",
    "# Access results for each view - Complete list of metric outputs\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    # Geometry outputs\n",
    "    pts3d = pred[\"pts3d\"]                     # 3D points in world coordinates (B, H, W, 3)\n",
    "    pts3d_cam = pred[\"pts3d_cam\"]             # 3D points in camera coordinates (B, H, W, 3)\n",
    "    depth_z = pred[\"depth_z\"]                 # Z-depth in camera frame (B, H, W, 1)\n",
    "    depth_along_ray = pred[\"depth_along_ray\"] # Depth along ray in camera frame (B, H, W, 1)\n",
    "\n",
    "    # Camera outputs\n",
    "    ray_directions = pred[\"ray_directions\"]   # Ray directions in camera frame (B, H, W, 3)\n",
    "    intrinsics = pred[\"intrinsics\"]           # Recovered pinhole camera intrinsics (B, 3, 3)\n",
    "    camera_poses = pred[\"camera_poses\"]       # OpenCV (+X - Right, +Y - Down, +Z - Forward) cam2world poses in world frame (B, 4, 4)\n",
    "    cam_trans = pred[\"cam_trans\"]             # OpenCV (+X - Right, +Y - Down, +Z - Forward) cam2world translation in world frame (B, 3)\n",
    "    cam_quats = pred[\"cam_quats\"]             # OpenCV (+X - Right, +Y - Down, +Z - Forward) cam2world quaternion in world frame (B, 4)\n",
    "\n",
    "    # Quality and masking\n",
    "    confidence = pred[\"conf\"]                 # Per-pixel confidence scores (B, H, W)\n",
    "    mask = pred[\"mask\"]                       # Combined validity mask (B, H, W, 1)\n",
    "    non_ambiguous_mask = pred[\"non_ambiguous_mask\"]                # Non-ambiguous regions (B, H, W)\n",
    "    non_ambiguous_mask_logits = pred[\"non_ambiguous_mask_logits\"]  # Mask logits (B, H, W)\n",
    "\n",
    "    # Scaling\n",
    "    metric_scaling_factor = pred[\"metric_scaling_factor\"]  # Applied metric scaling (B,)\n",
    "\n",
    "    # Original input\n",
    "    img_no_norm = pred[\"img_no_norm\"]         # Denormalized input images for visualization (B, H, W, 3)\n",
    "    confidence = pred[\"conf\"]                 # Per-pixel confidence scores (B, H, W)\n",
    "    mask = pred[\"mask\"]                       # Combined validity mask (B, H, W, 1)\n",
    "    non_ambiguous_mask = pred[\"non_ambiguous_mask\"]                # Non-ambiguous regions (B, H, W)\n",
    "    non_ambiguous_mask_logits = pred[\"non_ambiguous_mask_logits\"]  # Mask logits (B, H, W)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01ecc155-5a9c-4847-bf89-9cef0f7992e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions_to_numpy(predictions):\n",
    "    \"\"\"\n",
    "    Convert a dict of tensors to a dict of squeezed numpy arrays.\n",
    "\n",
    "    Args:\n",
    "        predictions (dict): Dictionary where values are torch tensors.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary with the same keys, values converted to numpy arrays\n",
    "              with squeezed dimensions.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        key: value.detach().cpu().numpy().squeeze()\n",
    "        for key, value in predictions.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb0acca6-4c7a-46cd-8ae9-9ea6cb29b406",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=[]\n",
    "for i, pred in enumerate(predictions):\n",
    "    d.append(predictions_to_numpy(pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128b8ead-03d5-4c4f-b87b-3f12e47ef579",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def save_preds_as_assignment_format(\n",
    "    preds,\n",
    "    output_dir,\n",
    "    base_name=\"frame\",\n",
    "    start_index=0\n",
    "):\n",
    "    \"\"\"\n",
    "    Save preds data in the assignment-required RGB-D format.\n",
    "\n",
    "    Args:\n",
    "        preds (list): List of dictionaries with prediction data.\n",
    "        output_dir (str): Directory where files will be saved.\n",
    "        base_name (str): Base filename (default: \"frame\").\n",
    "        start_index (int): Starting index for numbering (default: 0).\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        idx = start_index + i\n",
    "        idx_str = f\"{idx:04d}\"\n",
    "\n",
    "        # ---- Extract required fields ----\n",
    "        depth = pred[\"depth_z\"]\n",
    "        K = pred[\"intrinsics\"]\n",
    "        img = pred[\"img_no_norm\"]\n",
    "\n",
    "        # ---- Sanity checks ----\n",
    "        if depth.ndim != 2:\n",
    "            raise ValueError(f\"depth_z must be HxW, got {depth.shape}\")\n",
    "\n",
    "        if K.shape != (3, 3):\n",
    "            raise ValueError(f\"intrinsics must be 3x3, got {K.shape}\")\n",
    "\n",
    "       # ---- Handle image format ----        \n",
    "        \n",
    "        # CHW -> HWC if needed\n",
    "        if img.ndim == 3 and img.shape[0] == 3:\n",
    "            img = np.transpose(img, (1, 2, 0))\n",
    "        \n",
    "        # If image is float, rescale properly\n",
    "        if img.dtype != np.uint8:\n",
    "            img = img.astype(np.float32)\n",
    "        \n",
    "            # If values are in [0, 1], scale to [0, 255]\n",
    "            if img.max() <= 1.0:\n",
    "                img = img * 255.0\n",
    "        \n",
    "            img = np.clip(img, 0, 255).astype(np.uint8)\n",
    "        \n",
    "        # Convert RGB -> BGR for OpenCV\n",
    "        img_bgr = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "        # ---- File names ----\n",
    "        img_filename = f\"{base_name}_{idx_str}.jpg\"\n",
    "        depth_filename = f\"{base_name}_{idx_str}.mat\"\n",
    "\n",
    "        img_path = os.path.join(output_dir, img_filename)\n",
    "        depth_path = os.path.join(output_dir, depth_filename)\n",
    "\n",
    "        # ---- Save files ----\n",
    "        cv2.imwrite(img_path, img_bgr)\n",
    "\n",
    "        sio.savemat(\n",
    "            depth_path,\n",
    "            {\n",
    "                \"depth\": depth.astype(np.float32),\n",
    "                \"K\": K.astype(np.float32),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    print(f\"Saved {len(preds)} frames to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cf3bcfc0-f61d-4149-965f-ff6061da2c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 4 frames to /Users/jpc/Nextcloud/IST/Escolaridade/PIV2025/AulasPraticas/aula7_3D_registration/plondres/\n"
     ]
    }
   ],
   "source": [
    "output_dir=\"/Users/jpc/Nextcloud/IST/Escolaridade/PIV2025/AulasPraticas/aula7_3D_registration/plondres/\"\n",
    "# preds is your existing list of dictionaries\n",
    "\n",
    "save_preds_as_assignment_format(\n",
    "    preds=d,\n",
    "    output_dir=output_dir,\n",
    "    base_name=\"pl\",\n",
    "    start_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26003a14-6798-4a5e-b99e-211a5b0138c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "savemat(\"/Users/jpc/Nextcloud/IST/Escolaridade/PIV2025/AulasPraticas/aula7_3D_registration/plondres/plondres.mat\" ,{\"d\":d})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e294c6b5-247f-4790-9129-5e8db7d88dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
